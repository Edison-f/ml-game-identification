{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb1a4f1-bdc3-4a48-9ad5-a1d7f2017d61",
   "metadata": {},
   "source": [
    "## How Well Do the Models Do Their Jobs? - Edison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5839ff2-c278-4cf6-91b2-b88182269370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48a4b03b-9e98-4d1b-a78b-74ea07b589c8",
   "metadata": {},
   "source": [
    "## Visualizations - Angie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741eeb16-f463-426e-a23a-0bb0a362fe49",
   "metadata": {},
   "source": [
    "### Training vs. Testing Loss (Before Augmentation)\n",
    "![My Image](project_images/screenshot1.png)\n",
    "***Figure 1. Training and testing loss over 5 epochs before applying data augmentation.***\n",
    "\n",
    "Before augmentation, the training loss drops quickly while the test loss remains high and unstable, including a sharp spike at epoch 3. This behavior indicates overfitting, where the model memorizes the training data rather than learning generalizable features.\n",
    "\n",
    "### Training vs. Testing Loss (After Augmentation)\n",
    "![My Image](project_images/screenshot2.png)\n",
    "***Figure 2. Training and testing loss over 15 epochs after applying data augmentation.***\n",
    "\n",
    "After augmentation, the training loss still steadily decreases, but the test loss becomes more stable and shows fewer severe spikes. Although the test loss remains relatively high, the model is forced to generalize better because the augmented images add variability. This shows reduced overfitting compared to the first model, even though perfect generalization is not reached.\n",
    "\n",
    "### Predicted Labels for Validation Images\n",
    "![My Image](project_images/screenshot3.png)\n",
    "***Figure 3. Model predictions for images in the validation set. Each column corresponds to a different genre, and each image is labeled with the predicted class.***\n",
    "\n",
    "These predictions demonstrate how the trained model interprets visual features from new gameplay screenshots. Genres with distinctive visuals (such as puzzle games’ bright colors or strategy games’ overhead views) are classified more accurately. Errors typically occur in visually similar categories (e.g., RPG vs. shooter), highlighting the challenges of learning genre-specific visual patterns.\n",
    "\n",
    "### Confusion Matrix for Genre Classification\n",
    "![My Image](project_images/screenshot4.png)\n",
    "***Figure 4. Confusion matrix showing the number of validation images predicted for each genre. Darker colors indicate higher counts along the diagonal, representing correct predictions.***\n",
    "\n",
    "The confusion matrix shows that the model performs very well on puzzle, racing, and strategy images, with strong diagonal values indicating accurate classification. However, it struggles more with RPG and shooter images, which are sometimes confused with each other. This is likely due to similar camera perspectives or action-heavy scenes. Overall, the matrix highlights which genres are easiest to recognize and where the model’s weaknesses lie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f247556c-cde6-4d9d-8efa-71dada8dfd56",
   "metadata": {},
   "source": [
    "### RNN Prediction Grid (Pure Action, RPG, Strategy)\n",
    "![My Image](project_images/image.png)\n",
    "***Figure 5. Grid of RNN predictions on test images using three genres. Each tile shows the predicted label, true label, and the model’s output probabilities.***\n",
    "\n",
    "This figure shows the RNN’s predicted labels for test images after training on a subset of three genres. While the model correctly identifies several pure action it struggles significantly when distinguishing RPG from strategy.\n",
    "\n",
    "### RNN Loss Curve (pure action, rpg, strategy)\n",
    "![My Image](project_images/image2.png)\n",
    "***Figure 6. Training and testing loss curves for the RNN over 50 epochs. The training loss continually decreases while test loss begins rising, indicating overfitting.***\n",
    "\n",
    "This plot shows the RNN’s training and testing loss over 50 epochs for a subset of genres. The training curve steadily decreases, indicating the model is learning patterns from the training data. In contrast, the test loss begins increasing after ~15 epochs, revealing clear overfitting. \n",
    "\n",
    "### RNN Training and Testing Loss (Genres: shooter, puzzle, rpg, strategy, racing)\n",
    "![My Image](project_images/image3.png)\n",
    "***Figure 7. RNN training and testing loss curves for five genres. Test loss becomes unstable and rises early, indicating severe overfitting.***\n",
    "\n",
    "When trained on the full genre set, the RNN shows even stronger overfitting. Training loss decreases smoothly, but test loss rises sharply as training progresses. This indicates the RNN struggles to model the visual structure of screenshot data.\n",
    "\n",
    "### RNN Predicted Labels for Test Images (Genres: shooter, rpg, puzzle, racing, strategy)\n",
    "![My Image](project_images/image4.png)\n",
    "***Figure 8. Grid of RNN predictions on the five genres. Predictions include confidence probabilities and true labels for each test image.***\n",
    "\n",
    "The model shows several correct classifications for shooterimages, but the model frequently misclassifies genres such as RPG and strategy. Many predictions show incorrect labels. With five genres, the RNN struggles even more. \n",
    "\n",
    "### CNN Predicted Labels for Test Images (Genres: pure action, strategy, rpg)\n",
    "![My Image](project_images/image5.png)\n",
    "***Figure 9. Grid of CNN predictions for three genres. Each image shows the predicted and true labels along with output probabilities.***\n",
    "\n",
    "Compared to the RNN, the CNN makes fewer miclassifications. The model correctly identifies most pure action and strategy images.\n",
    "\n",
    "### CNN Training and Testing Loss (Genres: pure action, strategy, rpg)\n",
    "![My Image](project_images/image6.png)\n",
    "***Figure 10. CNN loss curves showing stable test loss and steadily decreasing training loss. The smaller gap between curves suggests good generalization.***\n",
    "\n",
    "Compared to the RNN, the CNN models training loss decreases steadily while test loss increases only mildly. Some overfitting is present, but its magnitude is smaller\n",
    "\n",
    "### CNN Training and Testing Loss (Genres: pure action, strategy, rpg)\n",
    "![My Image](project_images/image7.png)\n",
    "***Figure 11. CNN predictions on the five-genre dataset***\n",
    "\n",
    "The CNN handles the five-genre task far better than the RNN. Puzzle and shooter images are identified more often, while rpg, racing, and strategy still show confusion. \n",
    "\n",
    "### CNN Training and Testing Loss (Genres: shooter, puzzle, rpg, strategy, racing)\n",
    "![My Image](project_images/image8.png)\n",
    "***Figure 12. CNN training and testing loss curves for the five genres.***\n",
    "\n",
    "This plot displays the CNN’s training and testing loss across all five genres. Training loss decreases smoothly, while test loss fluctuates and rises after several epochs, indicating overfitting. However, the gap between curves is significantly smaller than the RNN's."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
